"""
è®­ç»ƒLOBSTERé¢„æµ‹å™¨

è®­ç»ƒæµç¨‹ï¼š
1. åŠ è½½JARVISå’ŒLOBSTERçš„é‡å æ ·æœ¬
2. è®­ç»ƒé¢„æµ‹å™¨å­¦ä¹ ICOHPå’ŒICOBI
3. éªŒè¯é¢„æµ‹è´¨é‡
4. ä¿å­˜æ¨¡å‹ç”¨äºç‰¹å¾ç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•ï¼š
    python train_lobster_predictor.py \
        --lobster_dir data/lobster_database \
        --overlap_map data/jarvis_mp_overlap.json \
        --output_dir models/lobster_predictor

ä½œè€…ï¼šClaude
æ—¥æœŸï¼š2025-12-10
"""

import os
import sys
import json
import argparse
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from models.lobster_predictor import (
    LOBSTERPredictorEnsemble,
    MultiTaskLOBSTERLoss,
    ICOHPPredictor
)
from utils.lobster_features import LobsterFeatureExtractor
from data import get_torch_dataset
from jarvis.db.figshare import data as jarvis_data


class LOBSTERPredictorDataset(torch.utils.data.Dataset):
    """LOBSTERé¢„æµ‹å™¨è®­ç»ƒæ•°æ®é›†

    æ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
    - æ™¶ä½“ç»“æ„å›¾ï¼ˆDGL graphï¼‰
    - çœŸå®çš„LOBSTERç‰¹å¾ï¼ˆä»JSONæå–ï¼‰
    """

    def __init__(self, jarvis_dataset, lobster_dir, overlap_map,
                 dataset_name='dft_3d', atom_features='cgcnn'):
        """åˆå§‹åŒ–

        Args:
            jarvis_dataset: JARVISæ•°æ®
            lobster_dir: LOBSTER JSONæ–‡ä»¶ç›®å½•
            overlap_map: {jarvis_id: mp_id} æ˜ å°„
            dataset_name: JARVISæ•°æ®é›†åç§°
            atom_features: åŸå­ç‰¹å¾ç±»å‹
        """
        self.jarvis_dataset = jarvis_dataset
        self.lobster_dir = Path(lobster_dir)
        self.overlap_map = overlap_map
        self.atom_features = atom_features

        # åªä¿ç•™æœ‰LOBSTERæ•°æ®çš„æ ·æœ¬
        self.valid_samples = []
        self.lobster_cache = {}

        print("åŠ è½½LOBSTERæ•°æ®...")
        for entry in tqdm(jarvis_dataset):
            jid = entry['jid']

            if jid in overlap_map:
                mp_id = overlap_map[jid]
                lobster_path = self.lobster_dir / f"{mp_id}.json"

                if lobster_path.exists():
                    # åŠ è½½LOBSTERç‰¹å¾
                    lobster_extractor = LobsterFeatureExtractor(
                        str(lobster_path)
                    )
                    self.lobster_cache[jid] = lobster_extractor
                    self.valid_samples.append(entry)

        print(f"âœ… åŠ è½½äº† {len(self.valid_samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")

        # æ„å»ºå›¾
        from graphs import Graph

        print("æ„å»ºæ™¶ä½“å›¾...")
        self.graphs = []
        for entry in tqdm(self.valid_samples):
            atoms = entry['atoms']

            # æ„å»ºDGLå›¾
            g, _ = Graph.atom_dgl_multigraph(
                atoms,
                cutoff=8.0,
                max_neighbors=12,
                atom_features=atom_features,
                compute_line_graph=False
            )

            self.graphs.append(g)

    def __len__(self):
        return len(self.valid_samples)

    def __getitem__(self, idx):
        """è·å–ä¸€ä¸ªæ ·æœ¬

        Returns:
            g: DGLå›¾
            lobster_targets: {
                'icohp': [num_edges, 1] çœŸå®ICOHP
                'icobi': [num_edges, 1] çœŸå®ICOBI
            }
        """
        entry = self.valid_samples[idx]
        jid = entry['jid']
        g = self.graphs[idx]

        # è·å–LOBSTERçœŸå®å€¼
        lobster = self.lobster_cache[jid]

        # ä¸ºæ¯æ¡è¾¹æå–LOBSTERç‰¹å¾
        src, dst = g.edges()
        num_edges = g.num_edges()

        icohp_values = []
        icobi_values = []

        for i, j in zip(src.numpy(), dst.numpy()):
            # è®¡ç®—è·ç¦»
            pos_i = g.ndata.get('pos', None)
            if pos_i is not None:
                pos_j = g.ndata['pos'][j]
                distance = torch.norm(pos_i - pos_j).item()
            else:
                # ä»è¾¹çš„ä½ç§»å‘é‡è®¡ç®—
                r = g.edata['r'][len(icohp_values)]
                distance = torch.norm(r).item()

            # è·å–LOBSTERç‰¹å¾
            lobster_feat = lobster.get_edge_features(i, j, distance)
            icohp_values.append(lobster_feat[0])
            icobi_values.append(lobster_feat[1])

        lobster_targets = {
            'icohp': torch.FloatTensor(icohp_values).unsqueeze(-1),
            'icobi': torch.FloatTensor(icobi_values).unsqueeze(-1)
        }

        return g, lobster_targets


def collate_fn(batch):
    """æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
    import dgl

    graphs = [item[0] for item in batch]
    targets = [item[1] for item in batch]

    # æ‰¹æ¬¡å›¾
    batched_graph = dgl.batch(graphs)

    # åˆå¹¶ç›®æ ‡
    batched_targets = {
        'icohp': torch.cat([t['icohp'] for t in targets], dim=0),
        'icobi': torch.cat([t['icobi'] for t in targets], dim=0)
    }

    return batched_graph, batched_targets


def train_epoch(model, train_loader, optimizer, criterion, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()

    total_loss = 0
    total_icohp_loss = 0
    total_icobi_loss = 0
    num_batches = 0

    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")

    for batch_idx, (g, targets) in enumerate(pbar):
        g = g.to(device)
        icohp_target = targets['icohp'].to(device)
        icobi_target = targets['icobi'].to(device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()

        icohp_pred, icohp_std, icobi_pred = model(g, return_uncertainty=True)

        # è®¡ç®—æŸå¤±
        loss, loss_dict = criterion(
            icohp_pred, icobi_pred,
            icohp_target, icobi_target,
            icohp_std
        )

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss_dict['total']
        total_icohp_loss += loss_dict['icohp']
        total_icobi_loss += loss_dict['icobi']
        num_batches += 1

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f"{loss_dict['total']:.4f}",
            'icohp': f"{loss_dict['icohp']:.4f}",
            'icobi': f"{loss_dict['icobi']:.4f}"
        })

    return {
        'total': total_loss / num_batches,
        'icohp': total_icohp_loss / num_batches,
        'icobi': total_icobi_loss / num_batches
    }


def validate(model, val_loader, criterion, device):
    """éªŒè¯"""
    model.eval()

    total_loss = 0
    total_icohp_loss = 0
    total_icobi_loss = 0
    num_batches = 0

    # ç”¨äºè®¡ç®—MAEå’Œç›¸å…³ç³»æ•°
    all_icohp_pred = []
    all_icohp_target = []
    all_icobi_pred = []
    all_icobi_target = []

    with torch.no_grad():
        for g, targets in tqdm(val_loader, desc="Validating"):
            g = g.to(device)
            icohp_target = targets['icohp'].to(device)
            icobi_target = targets['icobi'].to(device)

            # å‰å‘ä¼ æ’­
            icohp_pred, icohp_std, icobi_pred = model(
                g, return_uncertainty=True
            )

            # è®¡ç®—æŸå¤±
            loss, loss_dict = criterion(
                icohp_pred, icobi_pred,
                icohp_target, icobi_target,
                icohp_std
            )

            total_loss += loss_dict['total']
            total_icohp_loss += loss_dict['icohp']
            total_icobi_loss += loss_dict['icobi']
            num_batches += 1

            # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
            all_icohp_pred.append(icohp_pred.cpu())
            all_icohp_target.append(icohp_target.cpu())
            all_icobi_pred.append(icobi_pred.cpu())
            all_icobi_target.append(icobi_target.cpu())

    # åˆå¹¶ç»“æœ
    all_icohp_pred = torch.cat(all_icohp_pred)
    all_icohp_target = torch.cat(all_icohp_target)
    all_icobi_pred = torch.cat(all_icobi_pred)
    all_icobi_target = torch.cat(all_icobi_target)

    # è®¡ç®—æŒ‡æ ‡
    icohp_mae = torch.abs(all_icohp_pred - all_icohp_target).mean().item()
    icobi_mae = torch.abs(all_icobi_pred - all_icobi_target).mean().item()

    # è®¡ç®—ç›¸å…³ç³»æ•°
    icohp_corr = np.corrcoef(
        all_icohp_pred.numpy().flatten(),
        all_icohp_target.numpy().flatten()
    )[0, 1]

    icobi_corr = np.corrcoef(
        all_icobi_pred.numpy().flatten(),
        all_icobi_target.numpy().flatten()
    )[0, 1]

    return {
        'loss': {
            'total': total_loss / num_batches,
            'icohp': total_icohp_loss / num_batches,
            'icobi': total_icobi_loss / num_batches
        },
        'mae': {
            'icohp': icohp_mae,
            'icobi': icobi_mae
        },
        'correlation': {
            'icohp': icohp_corr,
            'icobi': icobi_corr
        }
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='è®­ç»ƒLOBSTERé¢„æµ‹å™¨'
    )

    parser.add_argument('--lobster_dir', type=str,
                       default='data/lobster_database',
                       help='LOBSTERæ•°æ®ç›®å½•')
    parser.add_argument('--overlap_map', type=str,
                       default='data/jarvis_mp_overlap.json',
                       help='JARVIS-MPé‡å æ˜ å°„æ–‡ä»¶')
    parser.add_argument('--dataset', type=str, default='dft_3d',
                       help='JARVISæ•°æ®é›†åç§°')
    parser.add_argument('--atom_features', type=str, default='cgcnn',
                       help='åŸå­ç‰¹å¾ç±»å‹')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--edge_hidden_dim', type=int, default=128,
                       help='è¾¹ç‰¹å¾éšè—å±‚ç»´åº¦')
    parser.add_argument('--graph_hidden_dim', type=int, default=256,
                       help='GNNéšè—å±‚ç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=4,
                       help='GNNå±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.1,
                       help='Dropoutç‡')
    parser.add_argument('--shared_encoder', action='store_true',
                       help='ä½¿ç”¨å…±äº«ç¼–ç å™¨')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=200,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')

    # è¾“å‡º
    parser.add_argument('--output_dir', type=str,
                       default='models/lobster_predictor',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--save_every', type=int, default=10,
                       help='æ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡')

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # TensorBoard
    writer = SummaryWriter(log_dir=str(output_dir / 'logs'))

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½JARVISæ•°æ®
    print("\nğŸ“Š åŠ è½½JARVISæ•°æ®...")
    jarvis_db = jarvis_data(args.dataset)

    # åŠ è½½é‡å æ˜ å°„
    print(f"ğŸ“‚ åŠ è½½é‡å æ˜ å°„: {args.overlap_map}")
    with open(args.overlap_map) as f:
        overlap_map = json.load(f)

    print(f"   é‡å æ ·æœ¬æ•°: {len(overlap_map)}")

    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ”¨ æ„å»ºè®­ç»ƒæ•°æ®é›†...")
    full_dataset = LOBSTERPredictorDataset(
        jarvis_dataset=jarvis_db,
        lobster_dir=args.lobster_dir,
        overlap_map=overlap_map,
        dataset_name=args.dataset,
        atom_features=args.atom_features
    )

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    val_size = int(len(full_dataset) * args.val_ratio)
    train_size = len(full_dataset) - val_size

    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )

    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=4,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=4,
        pin_memory=True
    )

    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = LOBSTERPredictorEnsemble(
        atom_feature_dim=92,
        edge_hidden_dim=args.edge_hidden_dim,
        graph_hidden_dim=args.graph_hidden_dim,
        num_layers=args.num_layers,
        dropout=args.dropout,
        shared_encoder=args.shared_encoder
    )

    model = model.to(device)

    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°é‡: {total_params:,}")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=10,
        verbose=True
    )

    # æŸå¤±å‡½æ•°
    criterion = MultiTaskLOBSTERLoss(
        icohp_weight=1.0,
        icobi_weight=1.0,
        use_uncertainty=True
    )

    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...\n")

    best_val_mae = float('inf')

    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_metrics = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch
        )

        # éªŒè¯
        val_metrics = validate(model, val_loader, criterion, device)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_metrics['loss']['total'])

        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('train/loss', train_metrics['total'], epoch)
        writer.add_scalar('train/icohp_loss', train_metrics['icohp'], epoch)
        writer.add_scalar('train/icobi_loss', train_metrics['icobi'], epoch)

        writer.add_scalar('val/loss', val_metrics['loss']['total'], epoch)
        writer.add_scalar('val/icohp_mae', val_metrics['mae']['icohp'], epoch)
        writer.add_scalar('val/icobi_mae', val_metrics['mae']['icobi'], epoch)
        writer.add_scalar('val/icohp_corr', val_metrics['correlation']['icohp'], epoch)
        writer.add_scalar('val/icobi_corr', val_metrics['correlation']['icobi'], epoch)

        # æ‰“å°ç»“æœ
        print(f"\nEpoch {epoch}/{args.epochs}")
        print(f"  Train Loss: {train_metrics['total']:.4f}")
        print(f"  Val Loss: {val_metrics['loss']['total']:.4f}")
        print(f"  ICOHP MAE: {val_metrics['mae']['icohp']:.4f} | "
              f"Corr: {val_metrics['correlation']['icohp']:.4f}")
        print(f"  ICOBI MAE: {val_metrics['mae']['icobi']:.4f} | "
              f"Corr: {val_metrics['correlation']['icobi']:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        avg_mae = (val_metrics['mae']['icohp'] + val_metrics['mae']['icobi']) / 2
        if avg_mae < best_val_mae:
            best_val_mae = avg_mae
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_metrics': val_metrics,
                'args': args
            }, output_dir / 'best_model.pt')
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (MAE: {avg_mae:.4f})")

        # å®šæœŸä¿å­˜
        if epoch % args.save_every == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_metrics': val_metrics,
                'args': args
            }, output_dir / f'checkpoint_epoch_{epoch}.pt')

    # è®­ç»ƒå®Œæˆ
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯MAE: {best_val_mae:.4f}")
    print(f"æ¨¡å‹ä¿å­˜è‡³: {output_dir}")
    print("="*60)

    writer.close()


if __name__ == '__main__':
    main()
