#python extract_alpha_local.py     --checkpoint bulk-new/output_100epochs_42_bs64_sw_ju_onlymiddle_bulk_modulus_kv_quantext/bulk_modulus_kv/best_test_model.pt         --root_dir /public/home/ghzhang/crysmmnet-main/dataset         --dataset jarvis         --property hse_bandgap-2         --n_samples 500         --output alpha_values.npz
import os
import sys
import csv
import argparse
import numpy as np
import torch
import dgl
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer

# æ·»åŠ æºä»£ç è·¯å¾„ (è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´)
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'crysmmnet-main/src'))

from jarvis.core.atoms import Atoms
from jarvis.core.graphs import Graph
from models.alignn import ALIGNN, MiddleFusionModule
from tokenizers.normalizers import BertNormalizer

# ==========================================
# 1. æ•°æ®åŠ è½½ä¸å¤„ç†
# ==========================================

def get_dataset_paths(root_dir, dataset, property_name):
    """è·å–æœ¬åœ°æ•°æ®é›†æ–‡ä»¶è·¯å¾„"""
    property_map = {
        'hse_bandgap': 'hse_bandgap', 'hse_bandgap-2': 'hse_bandgap-2',
        'bandgap_mbj': 'mbj_bandgap', 'formation_energy': 'formation_energy_peratom',
    }
    
    if dataset == 'jarvis':
        prop = property_map.get(property_name, property_name)
        return (os.path.join(root_dir, f'jarvis/{prop}/cif/'), 
                os.path.join(root_dir, f'jarvis/{prop}/description.csv'))
    elif dataset == 'mp':
        # æ ¹æ®ä½ çš„ç›®å½•ç»“æ„é€‚é… MP
        return (os.path.join(root_dir, 'mp_2018_new/'), 
                os.path.join(root_dir, 'mp_2018_new/mat_text.csv'))
    elif dataset == 'class':
        return (os.path.join(root_dir, f'class/{property_name}/cif/'), 
                os.path.join(root_dir, f'class/{property_name}/description.csv'))
    else:
        raise ValueError(f"Unknown dataset: {dataset}")

def load_local_data(cif_dir, csv_file, max_samples=None):
    """åŠ è½½ CSV å’Œ CIF"""
    print(f"è¯»å–æ•°æ®:\n  CIF: {cif_dir}\n  CSV: {csv_file}")
    
    with open(csv_file, 'r') as f:
        reader = csv.reader(f)
        try: headings = next(reader) 
        except: return []
        rows = [r for r in reader]
        
    data_list = []
    # ç®€å•çš„æ–‡æœ¬å½’ä¸€åŒ–
    norm = BertNormalizer(lowercase=True)
    
    print(f" å¼€å§‹å¤„ç† {min(len(rows), max_samples if max_samples else 99999)} ä¸ªæ ·æœ¬...")
    for row in tqdm(rows):
        if max_samples and len(data_list) >= max_samples: break
        
        try:
            # è§£æ CSV (æ ¹æ®ä¸åŒæ•°æ®é›†è°ƒæ•´ç´¢å¼•)
            # å‡è®¾ Jarvis æ ¼å¼: [id, formula, target, text, ...]
            # å‡è®¾ MP æ ¼å¼: [id, formula, target, ..., text, ...]
            if len(row) == 5: # Jarvis
                jid, target, text = row[0], row[2], row[3]
            elif len(row) >= 6: # MP
                jid, target, text = row[0], row[2], row[4]
            else:
                continue

            # è¯»å– CIF
            cif_path = os.path.join(cif_dir, f"{jid}.cif")
            if not os.path.exists(cif_path): 
                cif_path = os.path.join(cif_dir, jid) # å°è¯•æ— åç¼€
                if not os.path.exists(cif_path): continue
            
            atoms = Atoms.from_cif(cif_path)
            if atoms.num_atoms > 400: continue # è¿‡æ»¤è¿‡å¤§æ™¶ä½“

            data_list.append({
                'atoms': atoms,
                'text': norm.normalize_str(text),
                'target': float(target),
                'jid': jid
            })
        except Exception:
            continue
            
    return data_list

class SimpleDataset(Dataset):
    def __init__(self, data, tokenizer=None):
        self.data = data
        #self.tokenizer = tokenizer
        
    def __len__(self): return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        # æ„å›¾
        #g = Graph.atom_dgl_multigraph(item['atoms'], cutoff=8.0, max_neighbors=12)
        graph_output = Graph.atom_dgl_multigraph(item['atoms'], cutoff=8.0, max_neighbors=12)

        if isinstance(graph_output, tuple):
            g = graph_output[0]
            lg = graph_output[1]
        else:
            g = graph_output
            lg = g.line_graph(backtracking=False)

            # ç¡®ä¿æœ‰åŸå­åºæ•° Z (ç”¨äºåç»­æŒ‰å…ƒç´ åˆ†æ)
        if 'atom_types' not in g.ndata:
            if hasattr(item['atoms'], 'atomic_numbers'):
                z_vals = item['atoms'].atomic_numbers
            else:
                from jarvis.core.specie import Specie
                z_vals = [Specie(e).Z for e in item['atoms'].elements]
            #z_vals = [e.Z for e in item['atoms'].elements]
            g.ndata['atom_types'] = torch.tensor(z_vals).long()
            
        #lg = g.line_graph(backtracking=False)
        # æ–‡æœ¬
        #tokens = self.tokenizer(item['text'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')
        text = item['text']

        return g, lg, text, item['target'], item['jid']

def collate_fn(batch):
    gs, lgs, texts, targets, jids = zip(*batch)
    # ä¿å­˜ batch ä¸­æ¯ä¸ªå›¾çš„åŸå­åºæ•°ï¼Œä¾›åç»­æ‹†åˆ†ä½¿ç”¨
    atom_types_list = [g.ndata['atom_types'] for g in gs]
    batched_g = dgl.batch(gs)
    batched_lg = dgl.batch(lgs)
    batched_text = list(texts)
    batched_targets = torch.tensor(targets)
    return batched_g, batched_lg, batched_text, batched_targets, jids, atom_types_list

# ==========================================
# 2. ä¸»ç¨‹åº
# ==========================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', required=True)
    parser.add_argument('--root_dir', required=True)
    parser.add_argument('--output', default='alpha_values.npz')
    parser.add_argument('--n_samples', type=int, default=500)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--dataset', default='jarvis')
    parser.add_argument('--property', default='hse_bandgap-2')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1. åŠ è½½æ¨¡å‹
    print(f" åŠ è½½æ¨¡å‹: {args.checkpoint}")
    ckpt = torch.load(args.checkpoint, map_location=device, weights_only=False)
    model = ALIGNN(ckpt['config'])
    model.load_state_dict(ckpt['model'])
    model.to(device)
    model.eval()

    # 2. å®šä½ Fusion æ¨¡å— (ç”¨äºæå–å±æ€§)
    fusion_module = None
    for m in model.modules():
        if isinstance(m, MiddleFusionModule):
            fusion_module = m
            break
            
    if fusion_module is None:
        print("âŒ é”™è¯¯: æ¨¡å‹ä¸­æœªæ‰¾åˆ° MiddleFusionModuleï¼")
        return

    # 3. å‡†å¤‡æ•°æ®
    cif_dir, csv_file = get_dataset_paths(args.root_dir, args.dataset, args.property)
    raw_data = load_local_data(cif_dir, csv_file, args.n_samples)
    if not raw_data:
        print("âŒ æœªåŠ è½½åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return
        
    #tokenizer = AutoTokenizer.from_pretrained('m3rg-iitd/matscibert', model_max_length=512)

    loader = DataLoader(SimpleDataset(raw_data, tokenizer=None), 
                        batch_size=args.batch_size, 
                        collate_fn=collate_fn, 
                        shuffle=False, num_workers=0)

    # 4. æ¨ç†æå– Loop
    results = {'alphas': [], 'atom_types': [], 'targets': [], 'jids': []}
    
    print(" å¼€å§‹æå– Alpha å€¼...")
    with torch.no_grad():
        for batch in tqdm(loader):
            g, lg, text_list, targets, jids, atom_types_list = batch
            
            # Forward
            _ = model((g.to(device), lg.to(device), text_list))
            
            # === å…³é”®ï¼šä»æ¨¡å—å±æ€§ä¸­è¯»å– Alpha ===
            batch_alphas = None
            
            # ä¼˜å…ˆå°è¯•è¯»å– stored_alphas (æ¨èç‰ˆæœ¬)
            if hasattr(fusion_module, 'stored_alphas') and fusion_module.stored_alphas is not None:
                # åº”è¯¥æ˜¯ [Total_Atoms] çš„ CPU Tensor
                batch_alphas = fusion_module.stored_alphas.numpy()
                
            # å…¼å®¹å°è¯•è¯»å– gate_values (åŸå§‹ç‰ˆæœ¬)
            elif hasattr(fusion_module, 'gate_values') and fusion_module.gate_values is not None:
                # å¯èƒ½æ˜¯ [Total_Atoms, Hidden] çš„ GPU Tensor
                val = fusion_module.gate_values
                if val.dim() > 1: val = val.mean(dim=1) # å–å¹³å‡
                batch_alphas = val.detach().cpu().numpy()
            
            if batch_alphas is None:
                print("âš ï¸ è­¦å‘Š: æœªåœ¨æ¨¡å—ä¸­æ‰¾åˆ° stored_alphas æˆ– gate_valuesï¼Œè·³è¿‡æ­¤Batch")
                continue

            # === æ‹†åˆ† Batch ===
            batch_num_nodes = g.batch_num_nodes().cpu().numpy()
            idx = 0
            for i, n_atoms in enumerate(batch_num_nodes):
                # æå–å•ä¸ªæ™¶ä½“çš„ alpha
                c_alpha = batch_alphas[idx : idx + n_atoms]
                # æå–å•ä¸ªæ™¶ä½“çš„ Z (åŸå­åºæ•°)
                c_z = atom_types_list[i].numpy()
                
                results['alphas'].append(c_alpha)
                results['atom_types'].append(c_z)
                results['targets'].append(targets[i].item())
                results['jids'].append(jids[i])
                
                idx += n_atoms

    # 5. ä¿å­˜
    if len(results['alphas']) > 0:
        print(f" æ­£åœ¨æ‰“åŒ…æ•°æ® (å…± {len(results['alphas'])} ä¸ªæ ·æœ¬)...")

        # === æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ ===
        all_alphas = np.concatenate(results['alphas'])
        print(f"\nğŸ“Š Alpha å€¼ç»Ÿè®¡:")
        print(f"   - æ ·æœ¬æ•°: {len(results['alphas'])}")
        print(f"   - æ€»åŸå­æ•°: {len(all_alphas)}")
        print(f"   - å‡å€¼: {all_alphas.mean():.4f}")
        print(f"   - æ ‡å‡†å·®: {all_alphas.std():.4f}")
        print(f"   - æœ€å°å€¼: {all_alphas.min():.4f}")
        print(f"   - æœ€å¤§å€¼: {all_alphas.max():.4f}")
        print(f"   - 25%åˆ†ä½: {np.percentile(all_alphas, 25):.4f}")
        print(f"   - 50%åˆ†ä½: {np.percentile(all_alphas, 50):.4f}")
        print(f"   - 75%åˆ†ä½: {np.percentile(all_alphas, 75):.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é›†ä¸­
        if all_alphas.std() < 0.05:
            print(f"\n   âš ï¸  è­¦å‘Š: æ ‡å‡†å·®è¿‡å° ({all_alphas.std():.4f})ï¼ŒAlpha å€¼ç¼ºä¹å¤šæ ·æ€§!")
            print(f"      å¯èƒ½åŸå› :")
            print(f"      1. Gate ç½‘ç»œæƒé‡é¥±å’Œï¼ˆSigmoid è¾“å‡ºé›†ä¸­ï¼‰")
            print(f"      2. è¾“å…¥ç‰¹å¾ç¼ºä¹åŒºåˆ†åº¦")
            print(f"      3. æ¨¡å‹è®­ç»ƒä¸å……åˆ†")

        save_dict = {
            'alphas': np.array(results['alphas'], dtype=object),
            'atom_types': np.array(results['atom_types'], dtype=object),
            'targets': np.array(results['targets']), # æ ‡é‡ï¼Œä¸éœ€è¦ object
            'jids': np.array(results['jids'])        # å­—ç¬¦ä¸²/IDï¼Œä¸éœ€è¦ object
        }
        np.savez(args.output, **save_dict)
        print(f"\nâœ… æˆåŠŸä¿å­˜åˆ°: {args.output}")
    else:
        print("\nâŒ æå–å¤±è´¥: ç»“æœä¸ºç©ºã€‚")

if __name__ == '__main__':
    main()
