"""GPUæ˜¾å­˜ä½¿ç”¨ç›‘æ§å·¥å…·

ç”¨äºè¯Šæ–­ä¸åŒæ•°æ®é›†çš„æ˜¾å­˜å ç”¨å·®å¼‚
"""

import torch
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from data import get_train_val_loaders


def monitor_memory_usage(dataset_name, target, batch_size=64, num_batches=10):
    """ç›‘æ§æ•°æ®åŠ è½½çš„æ˜¾å­˜ä½¿ç”¨

    Args:
        dataset_name: æ•°æ®é›†åç§°
        target: ç›®æ ‡å±æ€§
        batch_size: æ‰¹æ¬¡å¤§å°
        num_batches: ç›‘æ§çš„æ‰¹æ¬¡æ•°é‡
    """

    print(f"\n{'='*80}")
    print(f"GPUæ˜¾å­˜ç›‘æ§ - {dataset_name}/{target}")
    print(f"{'='*80}\n")

    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ç›‘æ§GPUæ˜¾å­˜")
        return

    # æ¸…ç©ºæ˜¾å­˜
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()

    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    try:
        train_loader, val_loader, test_loader, _ = get_train_val_loaders(
            dataset=dataset_name,
            target=target,
            batch_size=batch_size,
            pin_memory=False,  # ç»Ÿä¸€ç”¨Falseé¿å…å¹²æ‰°
            workers=0  # ç»Ÿä¸€ç”¨0é¿å…å¤šè¿›ç¨‹å¹²æ‰°
        )
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
    print(f"   éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset)}")
    print(f"   Batch size: {batch_size}")
    print()

    # ç»Ÿè®¡å›¾å¤§å°
    node_counts = []
    edge_counts = []
    memory_usage = []

    print(f"ğŸ” åˆ†æå‰{num_batches}ä¸ªbatch...\n")

    device = torch.device('cuda')

    for i, batch in enumerate(train_loader):
        if i >= num_batches:
            break

        # è§£åŒ…batch
        if len(batch) == 4:
            g, lg, text, labels = batch
        elif len(batch) == 3:
            g, lg, labels = batch
            text = None
        else:
            g, labels = batch
            lg = None
            text = None

        # è®°å½•èŠ‚ç‚¹å’Œè¾¹æ•°
        nodes = g.num_nodes()
        edges = g.num_edges()
        node_counts.append(nodes)
        edge_counts.append(edges)

        # æ¸…ç©ºæ˜¾å­˜é‡æ–°å¼€å§‹
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        baseline_mem = torch.cuda.memory_allocated() / 1024**2

        # æ¨¡æ‹Ÿç§»åˆ°GPU
        g_gpu = g.to(device)
        if lg is not None:
            lg_gpu = lg.to(device)
        labels_gpu = labels.to(device)

        # è®°å½•æ˜¾å­˜
        current_mem = torch.cuda.memory_allocated() / 1024**2
        peak_mem = torch.cuda.max_memory_allocated() / 1024**2
        batch_mem = current_mem - baseline_mem

        memory_usage.append(batch_mem)

        print(f"Batch {i+1}/{num_batches}:")
        print(f"  èŠ‚ç‚¹æ•°: {nodes:>6,} ({nodes/batch_size:>5.1f} èŠ‚ç‚¹/æ ·æœ¬)")
        print(f"  è¾¹æ•°:   {edges:>6,} ({edges/batch_size:>5.1f} è¾¹/æ ·æœ¬)")
        print(f"  æ‰¹æ¬¡æ˜¾å­˜: {batch_mem:>6.1f} MB")
        print(f"  å³°å€¼æ˜¾å­˜: {peak_mem:>6.1f} MB")
        print()

        # æ¸…ç†
        del g_gpu, labels_gpu
        if lg is not None:
            del lg_gpu
        torch.cuda.empty_cache()

    # ç»Ÿè®¡æ‘˜è¦
    print(f"\n{'='*80}")
    print("ğŸ“Š ç»Ÿè®¡æ‘˜è¦")
    print(f"{'='*80}\n")

    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(train_loader.dataset):,}")
    print(f"  Batch size: {batch_size}")
    print(f"  æ€»batchæ•°: {len(train_loader)}")
    print()

    print(f"å›¾ç»“æ„ç»Ÿè®¡ (åŸºäº{num_batches}ä¸ªbatch):")
    print(f"  å¹³å‡èŠ‚ç‚¹æ•°/batch:  {np.mean(node_counts):>8.1f} Â± {np.std(node_counts):>6.1f}")
    print(f"  å¹³å‡è¾¹æ•°/batch:    {np.mean(edge_counts):>8.1f} Â± {np.std(edge_counts):>6.1f}")
    print(f"  å¹³å‡èŠ‚ç‚¹æ•°/æ ·æœ¬:   {np.mean(node_counts)/batch_size:>8.1f}")
    print(f"  å¹³å‡è¾¹æ•°/æ ·æœ¬:     {np.mean(edge_counts)/batch_size:>8.1f}")
    print(f"  èŠ‚ç‚¹æ•°èŒƒå›´:        {min(node_counts):>8,} ~ {max(node_counts):>8,}")
    print(f"  è¾¹æ•°èŒƒå›´:          {min(edge_counts):>8,} ~ {max(edge_counts):>8,}")
    print()

    print(f"æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡:")
    print(f"  å¹³å‡æ‰¹æ¬¡æ˜¾å­˜: {np.mean(memory_usage):>8.1f} MB Â± {np.std(memory_usage):>6.1f} MB")
    print(f"  æœ€å°æ‰¹æ¬¡æ˜¾å­˜: {min(memory_usage):>8.1f} MB")
    print(f"  æœ€å¤§æ‰¹æ¬¡æ˜¾å­˜: {max(memory_usage):>8.1f} MB")
    print()

    # ä¼°ç®—å®Œæ•´è®­ç»ƒæ˜¾å­˜
    model_size = 500  # MBï¼Œç²—ç•¥ä¼°è®¡
    optimizer_size = 1000  # MBï¼ŒAdamWçº¦2å€å‚æ•°
    gradient_size = 500  # MB
    activation_size = 800  # MBï¼Œç²—ç•¥ä¼°è®¡

    fixed_overhead = model_size + optimizer_size + gradient_size + activation_size
    avg_batch_mem = np.mean(memory_usage)
    total_estimated = fixed_overhead + avg_batch_mem

    print(f"é¢„ä¼°å®Œæ•´è®­ç»ƒæ˜¾å­˜:")
    print(f"  å›ºå®šå¼€é”€ (æ¨¡å‹+ä¼˜åŒ–å™¨+æ¢¯åº¦+æ¿€æ´»): ~{fixed_overhead:.0f} MB")
    print(f"  æ‰¹æ¬¡æ•°æ®å¼€é”€:                     ~{avg_batch_mem:.0f} MB")
    print(f"  æ€»è®¡:                             ~{total_estimated:.0f} MB ({total_estimated/1024:.2f} GB)")

    print(f"\n{'='*80}\n")

    return {
        'dataset': dataset_name,
        'target': target,
        'samples': len(train_loader.dataset),
        'batch_size': batch_size,
        'avg_nodes_per_batch': np.mean(node_counts),
        'avg_edges_per_batch': np.mean(edge_counts),
        'avg_nodes_per_sample': np.mean(node_counts) / batch_size,
        'avg_edges_per_sample': np.mean(edge_counts) / batch_size,
        'avg_batch_memory': np.mean(memory_usage),
        'estimated_total_memory': total_estimated,
    }


def compare_datasets(datasets, batch_size=64):
    """æ¯”è¾ƒå¤šä¸ªæ•°æ®é›†çš„æ˜¾å­˜ä½¿ç”¨

    Args:
        datasets: [(dataset_name, target, display_name), ...]
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    results = []

    for dataset, target, name in datasets:
        result = monitor_memory_usage(dataset, target, batch_size)
        if result:
            result['display_name'] = name
            results.append(result)

    if len(results) < 2:
        return

    # æ‰“å°å¯¹æ¯”
    print(f"\n{'='*120}")
    print(f"ğŸ“Š æ•°æ®é›†å¯¹æ¯”")
    print(f"{'='*120}\n")

    header = f"{'æ•°æ®é›†':<25} {'æ ·æœ¬æ•°':<10} {'èŠ‚ç‚¹/batch':<15} {'è¾¹/batch':<15} {'èŠ‚ç‚¹/æ ·æœ¬':<12} {'æ‰¹æ¬¡æ˜¾å­˜':<12} {'é¢„ä¼°æ€»æ˜¾å­˜':<12}"
    print(header)
    print("-" * 120)

    for r in results:
        print(f"{r['display_name']:<25} {r['samples']:<10} "
              f"{r['avg_nodes_per_batch']:<15.1f} {r['avg_edges_per_batch']:<15.1f} "
              f"{r['avg_nodes_per_sample']:<12.1f} "
              f"{r['avg_batch_memory']:<12.1f} {r['estimated_total_memory']/1024:<12.2f}")

    print()

    # è®¡ç®—å·®å¼‚
    if len(results) == 2:
        node_diff = (results[0]['avg_nodes_per_batch'] - results[1]['avg_nodes_per_batch'])
        node_diff_pct = node_diff / results[1]['avg_nodes_per_batch'] * 100

        mem_diff = (results[0]['avg_batch_memory'] - results[1]['avg_batch_memory'])
        mem_diff_pct = mem_diff / results[1]['avg_batch_memory'] * 100

        total_diff = (results[0]['estimated_total_memory'] - results[1]['estimated_total_memory'])
        total_diff_pct = total_diff / results[1]['estimated_total_memory'] * 100

        print("ğŸ“ˆ å·®å¼‚åˆ†æ:")
        print(f"  {results[0]['display_name']} vs {results[1]['display_name']}:")
        print(f"    èŠ‚ç‚¹æ•°/batch: {node_diff:+.1f} ({node_diff_pct:+.1f}%)")
        print(f"    æ‰¹æ¬¡æ˜¾å­˜:     {mem_diff:+.1f} MB ({mem_diff_pct:+.1f}%)")
        print(f"    é¢„ä¼°æ€»æ˜¾å­˜:   {total_diff:+.1f} MB ({total_diff_pct:+.1f}%)")
        print()

        if abs(node_diff_pct) > 10:
            print("ğŸ’¡ ç»“è®º: æ•°æ®é›†çš„å¹³å‡å›¾å¤§å°å·®å¼‚æ˜¾è‘—ï¼Œè¿™æ˜¯æ˜¾å­˜å·®å¼‚çš„ä¸»è¦åŸå› ï¼")
        elif abs(mem_diff_pct) < 5:
            print("ğŸ’¡ ç»“è®º: æ˜¾å­˜å·®å¼‚è¾ƒå°ï¼Œå¯èƒ½æ˜¯é…ç½®å‚æ•°ï¼ˆpin_memory/num_workersï¼‰å¯¼è‡´ã€‚")
        else:
            print("ğŸ’¡ ç»“è®º: æ˜¾å­˜å·®å¼‚ä¸­ç­‰ï¼Œå»ºè®®æ£€æŸ¥å®Œæ•´çš„è®­ç»ƒé…ç½®ã€‚")

    print(f"{'='*120}\n")


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='GPUæ˜¾å­˜ç›‘æ§å·¥å…·')
    parser.add_argument('--mode', type=str, default='single',
                       choices=['single', 'compare'],
                       help='è¿è¡Œæ¨¡å¼: single=ç›‘æ§å•ä¸ªæ•°æ®é›†, compare=å¯¹æ¯”å¤šä¸ªæ•°æ®é›†')
    parser.add_argument('--dataset', type=str, default='dft_3d',
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--target', type=str, default='hse_bandgap',
                       help='ç›®æ ‡å±æ€§')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_batches', type=int, default=10,
                       help='ç›‘æ§çš„æ‰¹æ¬¡æ•°é‡')

    args = parser.parse_args()

    if args.mode == 'single':
        monitor_memory_usage(
            args.dataset,
            args.target,
            args.batch_size,
            args.num_batches
        )
    elif args.mode == 'compare':
        # é¢„å®šä¹‰çš„å¯¹æ¯”ç»„
        datasets = [
            ('dft_3d', 'hse_bandgap', 'HSEå¸¦éš™ (1.6Kæ ·æœ¬)'),
            ('dft_3d', 'formation_energy_peratom', 'å½¢æˆèƒ½ (10K+æ ·æœ¬)'),
        ]
        compare_datasets(datasets, args.batch_size)
