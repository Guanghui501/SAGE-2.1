"""è¯Šæ–­è„šæœ¬ï¼šæ£€æŸ¥ alpha æå–å’Œæ–‡æœ¬æµåˆ†æçš„é—®é¢˜

ä½¿ç”¨æ–¹æ³•:
python debug_alpha_extraction.py --checkpoint <path> --root_dir <path>
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'crysmmnet-main/src'))
from models.alignn import ALIGNN, MiddleFusionModule

# å¤ç”¨æ•°æ®åŠ è½½
from extract_alpha_final import SimpleDataset, load_local_data, get_dataset_paths, collate_fn

def diagnose_model(checkpoint_path, root_dir):
    """è¯Šæ–­æ¨¡å‹å’Œæ•°æ®"""

    print("=" * 80)
    print("ğŸ” Alpha æå–è¯Šæ–­å·¥å…·")
    print("=" * 80)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ“ è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # æ£€æŸ¥ config
    if 'config' not in ckpt:
        print("âŒ é”™è¯¯: checkpoint ä¸­æ²¡æœ‰ 'config' é”®")
        print(f"   å¯ç”¨çš„é”®: {list(ckpt.keys())}")
        return

    config = ckpt['config']
    print(f"âœ… Config åŠ è½½æˆåŠŸ")
    print(f"   - use_middle_fusion: {config.use_middle_fusion}")
    print(f"   - middle_fusion_layers: {config.middle_fusion_layers if config.use_middle_fusion else 'N/A'}")

    if not config.use_middle_fusion:
        print("\nâŒ è‡´å‘½é”™è¯¯: æ¨¡å‹æœªå¯ç”¨ middle_fusion!")
        print("   è¯·ä½¿ç”¨å¯ç”¨äº† middle_fusion çš„æ¨¡å‹checkpoint")
        return

    model = ALIGNN(config)
    model.load_state_dict(ckpt['model'])
    model.to(device)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    # 2. æŸ¥æ‰¾ Fusion æ¨¡å—
    print("\nğŸ” æ£€æŸ¥ MiddleFusionModule...")
    fusion_modules = []
    for name, module in model.named_modules():
        if isinstance(module, MiddleFusionModule):
            fusion_modules.append((name, module))
            print(f"   âœ… æ‰¾åˆ°: {name}")
            print(f"      - node_dim: {module.node_dim}")
            print(f"      - text_dim: {module.text_dim}")
            print(f"      - hidden_dim: {module.hidden_dim}")

    if not fusion_modules:
        print("   âŒ é”™è¯¯: æœªæ‰¾åˆ° MiddleFusionModule!")
        return

    fusion_module = fusion_modules[0][1]  # ä½¿ç”¨ç¬¬ä¸€ä¸ª

    # 3. åŠ è½½å°‘é‡æ•°æ®
    print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    cif_dir, csv_file = get_dataset_paths(root_dir, 'jarvis', 'hse_bandgap-2')
    raw_data = load_local_data(cif_dir, csv_file, max_samples=5)

    if not raw_data:
        print("âŒ é”™è¯¯: æ— æ³•åŠ è½½æ•°æ®")
        return

    print(f"âœ… åŠ è½½äº† {len(raw_data)} ä¸ªæ ·æœ¬")

    loader = DataLoader(SimpleDataset(raw_data, tokenizer=None),
                       batch_size=2,
                       collate_fn=collate_fn)

    # 4. å‰å‘ä¼ æ’­å¹¶æ£€æŸ¥
    print("\nğŸš€ æ‰§è¡Œå‰å‘ä¼ æ’­...")

    batch = next(iter(loader))
    g, lg, text_list, targets, jids, atom_types_list = batch

    print(f"   - Batch size: {len(text_list)}")
    print(f"   - å›¾èŠ‚ç‚¹æ•°: {g.batch_num_nodes().tolist()}")
    print(f"   - æ–‡æœ¬ç¤ºä¾‹: '{text_list[0][:100]}...'")

    # === å…³é”®: æ·»åŠ  Hook æ•è·ä¸­é—´å€¼ ===
    captured_data = {}

    def capture_fusion_input(module, input_tuple, output):
        """æ•è· fusion æ¨¡å—çš„è¾“å…¥å’Œè¾“å‡º"""
        node_feat, text_feat, batch_num_nodes = input_tuple
        captured_data['node_feat_shape'] = node_feat.shape
        captured_data['text_feat_shape'] = text_feat.shape
        captured_data['node_feat_norm'] = node_feat.norm(dim=1).mean().item()
        captured_data['text_feat_norm'] = text_feat.norm(dim=1).mean().item()
        captured_data['output_norm'] = output.norm(dim=1).mean().item()

    def capture_gate_values(module, input_tuple, output):
        """æ•è· gate çš„è¾“å‡º"""
        # gate æ˜¯ä¸€ä¸ª Sequentialï¼Œæˆ‘ä»¬Hookå®ƒçš„è¾“å‡º
        captured_data['gate_output'] = output.detach()

    # æ³¨å†Œ Hooks
    fusion_hook = fusion_module.register_forward_hook(capture_fusion_input)
    gate_hook = fusion_module.gate.register_forward_hook(capture_gate_values)

    with torch.no_grad():
        _ = model((g.to(device), lg.to(device), text_list))

    fusion_hook.remove()
    gate_hook.remove()

    # 5. æ£€æŸ¥æ•è·çš„æ•°æ®
    print("\nğŸ“ˆ Fusion æ¨¡å—è¯Šæ–­:")
    print(f"   - èŠ‚ç‚¹ç‰¹å¾å½¢çŠ¶: {captured_data.get('node_feat_shape', 'N/A')}")
    print(f"   - æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {captured_data.get('text_feat_shape', 'N/A')}")
    print(f"   - èŠ‚ç‚¹ç‰¹å¾L2èŒƒæ•°(å‡å€¼): {captured_data.get('node_feat_norm', 0):.4f}")
    print(f"   - æ–‡æœ¬ç‰¹å¾L2èŒƒæ•°(å‡å€¼): {captured_data.get('text_feat_norm', 0):.4f}")
    print(f"   - è¾“å‡ºç‰¹å¾L2èŒƒæ•°(å‡å€¼): {captured_data.get('output_norm', 0):.4f}")

    # 6. æ£€æŸ¥ stored_alphas
    print("\nğŸ” æ£€æŸ¥ stored_alphas:")
    if hasattr(fusion_module, 'stored_alphas') and fusion_module.stored_alphas is not None:
        alphas = fusion_module.stored_alphas.numpy()
        print(f"   âœ… stored_alphas å½¢çŠ¶: {alphas.shape}")
        print(f"   - å‡å€¼: {alphas.mean():.4f}")
        print(f"   - æ ‡å‡†å·®: {alphas.std():.4f}")
        print(f"   - æœ€å°å€¼: {alphas.min():.4f}")
        print(f"   - æœ€å¤§å€¼: {alphas.max():.4f}")
        print(f"   - 25%åˆ†ä½: {np.percentile(alphas, 25):.4f}")
        print(f"   - 50%åˆ†ä½: {np.percentile(alphas, 50):.4f}")
        print(f"   - 75%åˆ†ä½: {np.percentile(alphas, 75):.4f}")
    else:
        print("   âŒ stored_alphas ä¸ºç©º!")

    # 7. æ£€æŸ¥ gate_values çš„åŸå§‹å€¼
    if 'gate_output' in captured_data:
        gate_vals = captured_data['gate_output']
        print(f"\nğŸ” æ£€æŸ¥åŸå§‹ gate_values (Sigmoid å):")
        print(f"   - å½¢çŠ¶: {gate_vals.shape}")
        print(f"   - å‡å€¼: {gate_vals.mean().item():.4f}")
        print(f"   - æ ‡å‡†å·®: {gate_vals.std().item():.4f}")
        print(f"   - æœ€å°å€¼: {gate_vals.min().item():.4f}")
        print(f"   - æœ€å¤§å€¼: {gate_vals.max().item():.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦é¥±å’Œ
        if gate_vals.min().item() > 0.3 and gate_vals.max().item() < 0.5:
            print("   âš ï¸  è­¦å‘Š: Gate å€¼èŒƒå›´å¤ªçª„ (0.3-0.5)ï¼Œå¯èƒ½å‘ç”Ÿäº†é¥±å’Œ!")
            print("      å»ºè®®æ£€æŸ¥:")
            print("      1. Gate ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–")
            print("      2. è¾“å…¥ç‰¹å¾çš„å½’ä¸€åŒ–")
            print("      3. è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ ç‡")

    # 8. åˆ†ææ–‡æœ¬ç‰¹å¾å’ŒèŠ‚ç‚¹ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦
    print("\nğŸ” è®¡ç®—æ–‡æœ¬-èŠ‚ç‚¹ä½™å¼¦ç›¸ä¼¼åº¦:")
    node_feat_raw = captured_data.get('node_feat_shape')
    text_feat_raw = captured_data.get('text_feat_shape')

    # é‡æ–°è¿è¡Œä¸€æ¬¡ä»¥è·å–å®é™…tensorï¼ˆä¹‹å‰åªä¿å­˜äº†shapeï¼‰
    captured_tensors = {}

    def capture_tensors(module, input_tuple, output):
        node_feat, text_feat, batch_num_nodes = input_tuple
        captured_tensors['node_feat'] = node_feat.detach()
        captured_tensors['text_feat'] = text_feat.detach()
        captured_tensors['batch_num_nodes'] = batch_num_nodes

    hook = fusion_module.register_forward_hook(capture_tensors)
    with torch.no_grad():
        _ = model((g.to(device), lg.to(device), text_list))
    hook.remove()

    if 'node_feat' in captured_tensors and 'text_feat' in captured_tensors:
        node_feat = captured_tensors['node_feat']
        text_feat = captured_tensors['text_feat']
        batch_num_nodes = captured_tensors['batch_num_nodes']

        # å¹¿æ’­textç‰¹å¾åˆ°èŠ‚ç‚¹
        text_expanded = []
        for i, num in enumerate(batch_num_nodes):
            text_expanded.append(text_feat[i].unsqueeze(0).repeat(num, 1))
        text_broadcasted = torch.cat(text_expanded, dim=0)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cos_sim = F.cosine_similarity(node_feat, text_broadcasted, dim=1)
        print(f"   - ä½™å¼¦ç›¸ä¼¼åº¦å‡å€¼: {cos_sim.mean().item():.4f}")
        print(f"   - ä½™å¼¦ç›¸ä¼¼åº¦æ ‡å‡†å·®: {cos_sim.std().item():.4f}")
        print(f"   - ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´: [{cos_sim.min().item():.4f}, {cos_sim.max().item():.4f}]")

        if abs(cos_sim.mean().item()) < 0.1:
            print("   âš ï¸  è­¦å‘Š: ä½™å¼¦ç›¸ä¼¼åº¦æä½! å¯èƒ½åŸå› :")
            print("      1. æ–‡æœ¬å’Œå›¾ç‰¹å¾åœ¨ä¸åŒçš„å‘é‡ç©ºé—´")
            print("      2. ç‰¹å¾å½’ä¸€åŒ–é—®é¢˜")
            print("      3. æ–‡æœ¬ç‰¹å¾æ²¡æœ‰æœ‰æ•ˆèå…¥å›¾ç¼–ç ")

    print("\n" + "=" * 80)
    print("âœ… è¯Šæ–­å®Œæˆ")
    print("=" * 80)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è¯Šæ–­ alpha æå–é—®é¢˜")
    parser.add_argument('--checkpoint', required=True, help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--root_dir', required=True, help='æ•°æ®é›†æ ¹ç›®å½•')
    args = parser.parse_args()

    diagnose_model(args.checkpoint, args.root_dir)
