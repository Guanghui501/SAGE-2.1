#!/usr/bin/env python
"""
Gated Cross-Attention è¯Šæ–­å·¥å…·

ç”¨äºè¯Šæ–­å’Œåˆ†æå¸¦æœ‰é—¨æ§è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ã€‚

ç”¨æ³•:
    python diagnose_gated_attention.py --checkpoint path/to/model.pt [--dataset jarvis/mbj_bandgap]
"""

import os
import sys
import argparse
import torch
import numpy as np
from pathlib import Path
from transformers import AutoTokenizer, AutoModel

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'crysmmnet-main/src'))

from data import get_train_val_loaders
from models.alignn import ALIGNN, ALIGNNConfig


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Gated Cross-Attention è¯Šæ–­å·¥å…·')
    parser.add_argument('--checkpoint', type=str, required=True,
                      help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, default='jarvis/mbj_bandgap',
                      help='æ•°æ®é›†åç§° (å¦‚: jarvis/mbj_bandgap, jarvis/formation_energy_peratom)')
    parser.add_argument('--batch_size', type=int, default=32,
                      help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--n_samples', type=int, default=10,
                      help='è¦åˆ†æçš„æ ·æœ¬æ•°')

    return parser.parse_args()


def load_model(checkpoint_path):
    """åŠ è½½æ¨¡å‹checkpoint"""
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {checkpoint_path}")

    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°checkpointæ–‡ä»¶: {checkpoint_path}")

    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    # ä»checkpointä¸­æå–é…ç½®
    if 'config' in checkpoint:
        config = checkpoint['config']
    else:
        # å¦‚æœæ²¡æœ‰ä¿å­˜é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        print("âš ï¸  Checkpointä¸­æœªæ‰¾åˆ°é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = ALIGNNConfig(
            name="alignn",
            alignn_layers=4,
            gcn_layers=4,
            hidden_features=256,
            use_cross_modal_attention=True,
            cross_modal_num_heads=4,
            cross_modal_hidden_dim=256,
            cross_modal_dropout=0.1
        )

    # åˆ›å»ºæ¨¡å‹
    model = ALIGNN(config)

    # åŠ è½½æƒé‡
    if 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        model.load_state_dict(checkpoint)

    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    return model, config


def analyze_model_config(model, config):
    """åˆ†ææ¨¡å‹é…ç½®"""
    print("\n" + "="*80)
    print("ğŸ“Š æ¨¡å‹é…ç½®åˆ†æ")
    print("="*80)

    print(f"\nğŸ”§ åŸºç¡€é…ç½®:")
    print(f"   - ALIGNNå±‚æ•°: {config.alignn_layers}")
    print(f"   - GCNå±‚æ•°: {config.gcn_layers}")
    print(f"   - éšè—å±‚ç»´åº¦: {config.hidden_features}")
    print(f"   - è¾“å‡ºç»´åº¦: {config.output_features}")

    print(f"\nğŸ”€ è·¨æ¨¡æ€æ³¨æ„åŠ›é…ç½®:")
    print(f"   - ä½¿ç”¨Gated Cross-Attention: {config.use_cross_modal_attention}")

    if config.use_cross_modal_attention:
        print(f"   - æ³¨æ„åŠ›å¤´æ•°: {config.cross_modal_num_heads}")
        print(f"   - éšè—å±‚ç»´åº¦: {config.cross_modal_hidden_dim}")
        print(f"   - Dropoutç‡: {config.cross_modal_dropout}")

        # æ£€æŸ¥cross_modal_attentionæ¨¡å—
        if hasattr(model, 'cross_modal_attention'):
            print(f"\n   âœ… Cross-Modal Attention æ¨¡å—å·²å¯ç”¨")
            print(f"      - Graphç»´åº¦: 64")
            print(f"      - Textç»´åº¦: 64")
        else:
            print(f"\n   âš ï¸  æ¨¡å‹ä¸­æœªæ‰¾åˆ° cross_modal_attention æ¨¡å—")

    print(f"\nğŸ”¬ ç»†ç²’åº¦æ³¨æ„åŠ›é…ç½®:")
    print(f"   - ä½¿ç”¨Fine-Grained Attention: {getattr(config, 'use_fine_grained_attention', False)}")

    if getattr(config, 'use_fine_grained_attention', False):
        print(f"   - æ³¨æ„åŠ›å¤´æ•°: {config.fine_grained_num_heads}")
        print(f"   - éšè—å±‚ç»´åº¦: {config.fine_grained_hidden_dim}")

    print(f"\nğŸ”„ ä¸­æœŸèåˆé…ç½®:")
    print(f"   - ä½¿ç”¨Middle Fusion: {getattr(config, 'use_middle_fusion', False)}")

    if getattr(config, 'use_middle_fusion', False):
        print(f"   - èåˆå±‚ç´¢å¼•: {config.middle_fusion_layers}")
        print(f"   - æ³¨æ„åŠ›å¤´æ•°: {config.middle_fusion_num_heads}")


def load_data(dataset_name, batch_size=32):
    """åŠ è½½æ•°æ®é›†"""
    print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_name}")

    try:
        # è§£ææ•°æ®é›†åç§°
        if '/' in dataset_name:
            # æ ¼å¼: jarvis/property_name
            parts = dataset_name.split('/')
            dataset = parts[0]
            target = parts[1] if len(parts) > 1 else 'formation_energy_peratom'
        else:
            dataset = dataset_name
            target = 'formation_energy_peratom'

        # è°ƒç”¨ get_train_val_loaders (ä¸ä¼ å…¥ root_dir!)
        train_loader, val_loader, test_loader, prepare_batch = get_train_val_loaders(
            dataset=dataset,
            target=target,
            batch_size=batch_size,
            atom_features="cgcnn",
            neighbor_strategy="k-nearest",
            id_tag="jid",
            pin_memory=False,
            workers=0,
            save_dataloader=False,
            use_canonize=True,
            filename=f"temp_{dataset}_{target}",
            cutoff=8.0,
            max_neighbors=12,
            val_ratio=0.1,
            test_ratio=0.1,
        )

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   - è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"   - éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        print(f"   - æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")

        return train_loader, val_loader, test_loader, prepare_batch

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None


def analyze_attention_weights(model, test_loader, device, n_samples=10):
    """åˆ†ææ³¨æ„åŠ›æƒé‡"""
    print("\n" + "="*80)
    print("ğŸ” åˆ†ææ³¨æ„åŠ›æƒé‡")
    print("="*80)

    if not hasattr(model, 'cross_modal_attention'):
        print("âš ï¸  æ¨¡å‹æœªä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›")
        return

    model.eval()
    all_g2t_weights = []
    all_t2g_weights = []

    print(f"\nğŸ“¦ æ”¶é›† {n_samples} ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡...")

    count = 0
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if count >= n_samples:
                break

            g, lg, text, labels = batch

            # Forward pass with attention tracking
            output = model(
                [g.to(device), lg.to(device), text],
                return_attention=True,
                return_features=True
            )

            if isinstance(output, dict) and 'attention_weights' in output:
                attn = output['attention_weights']

                if attn is not None:
                    if 'graph_to_text' in attn and attn['graph_to_text'] is not None:
                        all_g2t_weights.append(attn['graph_to_text'].cpu())

                    if 'text_to_graph' in attn and attn['text_to_graph'] is not None:
                        all_t2g_weights.append(attn['text_to_graph'].cpu())

            count += len(labels)
            print(f"   è¿›åº¦: {count}/{n_samples}", end='\r')

    print(f"\nâœ… æ”¶é›†å®Œæˆ: {count} ä¸ªæ ·æœ¬")

    # åˆ†æç»Ÿè®¡
    if all_g2t_weights:
        g2t = torch.cat(all_g2t_weights, dim=0)
        print(f"\nğŸ“Š Graphâ†’Text æ³¨æ„åŠ›ç»Ÿè®¡:")
        print(f"   - å½¢çŠ¶: {g2t.shape}")
        print(f"   - å¹³å‡å€¼: {g2t.mean():.4f}")
        print(f"   - æ ‡å‡†å·®: {g2t.std():.4f}")
        print(f"   - æœ€å°å€¼: {g2t.min():.4f}")
        print(f"   - æœ€å¤§å€¼: {g2t.max():.4f}")

        # æŒ‰æ³¨æ„åŠ›å¤´åˆ†æ
        if g2t.dim() >= 2:
            num_heads = g2t.shape[1]
            print(f"\n   å„æ³¨æ„åŠ›å¤´ç»Ÿè®¡:")
            for head_idx in range(num_heads):
                head_weights = g2t[:, head_idx]
                print(f"      Head {head_idx}: å‡å€¼={head_weights.mean():.4f}, "
                      f"æ ‡å‡†å·®={head_weights.std():.4f}")

    if all_t2g_weights:
        t2g = torch.cat(all_t2g_weights, dim=0)
        print(f"\nğŸ“Š Textâ†’Graph æ³¨æ„åŠ›ç»Ÿè®¡:")
        print(f"   - å½¢çŠ¶: {t2g.shape}")
        print(f"   - å¹³å‡å€¼: {t2g.mean():.4f}")
        print(f"   - æ ‡å‡†å·®: {t2g.std():.4f}")
        print(f"   - æœ€å°å€¼: {t2g.min():.4f}")
        print(f"   - æœ€å¤§å€¼: {t2g.max():.4f}")

        # æŒ‰æ³¨æ„åŠ›å¤´åˆ†æ
        if t2g.dim() >= 2:
            num_heads = t2g.shape[1]
            print(f"\n   å„æ³¨æ„åŠ›å¤´ç»Ÿè®¡:")
            for head_idx in range(num_heads):
                head_weights = t2g[:, head_idx]
                print(f"      Head {head_idx}: å‡å€¼={head_weights.mean():.4f}, "
                      f"æ ‡å‡†å·®={head_weights.std():.4f}")

    return all_g2t_weights, all_t2g_weights


def analyze_predictions(model, test_loader, device, n_samples=10):
    """åˆ†ææ¨¡å‹é¢„æµ‹"""
    print("\n" + "="*80)
    print("ğŸ¯ åˆ†ææ¨¡å‹é¢„æµ‹")
    print("="*80)

    model.eval()
    all_predictions = []
    all_labels = []
    all_errors = []

    print(f"\nğŸ“¦ æ”¶é›† {n_samples} ä¸ªæ ·æœ¬çš„é¢„æµ‹...")

    count = 0
    with torch.no_grad():
        for batch in test_loader:
            if count >= n_samples:
                break

            g, lg, text, labels = batch

            # Forward pass
            output = model([g.to(device), lg.to(device), text])

            if isinstance(output, dict):
                predictions = output['predictions']
            else:
                predictions = output

            predictions = predictions.cpu().squeeze()
            labels = labels.cpu().squeeze()

            all_predictions.append(predictions)
            all_labels.append(labels)
            all_errors.append(torch.abs(predictions - labels))

            count += len(labels)
            print(f"   è¿›åº¦: {count}/{n_samples}", end='\r')

    print(f"\nâœ… æ”¶é›†å®Œæˆ: {count} ä¸ªæ ·æœ¬")

    # åˆå¹¶ç»“æœ
    predictions = torch.cat(all_predictions)
    labels = torch.cat(all_labels)
    errors = torch.cat(all_errors)

    # è®¡ç®—æŒ‡æ ‡
    mae = errors.mean().item()
    rmse = torch.sqrt((errors ** 2).mean()).item()

    print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    print(f"   - MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f}")
    print(f"   - RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f}")
    print(f"   - é¢„æµ‹èŒƒå›´: [{predictions.min():.4f}, {predictions.max():.4f}]")
    print(f"   - çœŸå®å€¼èŒƒå›´: [{labels.min():.4f}, {labels.max():.4f}]")

    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
    print(f"\nğŸ“‹ æ ·æœ¬é¢„æµ‹ (å‰10ä¸ª):")
    print(f"{'='*60}")
    print(f"{'æ ·æœ¬':<8} {'çœŸå®å€¼':<12} {'é¢„æµ‹å€¼':<12} {'è¯¯å·®':<12}")
    print(f"{'-'*60}")
    for i in range(min(10, len(predictions))):
        print(f"{i:<8} {labels[i]:<12.4f} {predictions[i]:<12.4f} {errors[i]:<12.4f}")
    print(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    print("\n" + "="*80)
    print("Gated Cross-Attention è¯Šæ–­å·¥å…·")
    print("="*80)

    # 1. åŠ è½½æ¨¡å‹
    model, config = load_model(args.checkpoint)

    # 2. åˆ†ææ¨¡å‹é…ç½®
    analyze_model_config(model, config)

    # 3. åŠ è½½æ•°æ®
    train_loader, val_loader, test_loader, prepare_batch = load_data(
        args.dataset,
        batch_size=args.batch_size
    )

    if test_loader is None:
        print("\nâŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œè¯Šæ–­ç»ˆæ­¢")
        return

    # 4. è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # 5. åˆ†ææ³¨æ„åŠ›æƒé‡
    if config.use_cross_modal_attention:
        analyze_attention_weights(model, test_loader, device, n_samples=args.n_samples)

    # 6. åˆ†æé¢„æµ‹
    analyze_predictions(model, test_loader, device, n_samples=args.n_samples)

    print("\n" + "="*80)
    print("âœ… è¯Šæ–­å®Œæˆ")
    print("="*80)


if __name__ == '__main__':
    main()
