# ğŸš¨ ç´§æ€¥ä¿®å¤ï¼štext_transform è¾“å‡ºè¿‡å¼±å¯¼è‡´èåˆå¤±æ•ˆ

## è¯Šæ–­ç»“æœæ€»ç»“

è¿è¡Œ `diagnose_fusion_effectiveness.py` å‘ç°äº†ä¸¥é‡é—®é¢˜ï¼š

```
text_transform è¾“å…¥:  L2 = 8.30, std = 0.148
text_transform è¾“å‡º:  L2 = 2.17, std = 0.054  â† ä¸‹é™ 74%ï¼

èŠ‚ç‚¹ç‰¹å¾:  L2 = 27.43
æ–‡æœ¬ç‰¹å¾:  L2 =  2.17
æ¯”ä¾‹: 12.63:1  â† æåº¦ä¸å¹³è¡¡ï¼

Gate å€¼ä¸èåˆå˜åŒ–ç›¸å…³æ€§: -0.70  â† è´Ÿç›¸å…³ï¼ˆå¼‚å¸¸ï¼‰
```

**æ ¹æœ¬é—®é¢˜**: text_transform çš„è¾“å‡ºå¤ªå¼±ï¼Œæ— æ³•ä¸èŠ‚ç‚¹ç‰¹å¾ç«äº‰ã€‚

---

## ä¿®å¤æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | éš¾åº¦ | æ˜¯å¦éœ€è¦é‡è®­ç»ƒ | æ•ˆæœ | æ¨èåº¦ |
|------|------|--------------|------|--------|
| **A. ç¼©æ”¾ text ç‰¹å¾** | â­ | âŒ | ä¸­ | â­â­â­ |
| **B. æ·»åŠ  LayerNorm** | â­â­ | âœ… | é«˜ | â­â­â­â­â­ |
| **C. ä¿®æ”¹ text_transform åˆå§‹åŒ–** | â­â­ | âœ… | é«˜ | â­â­â­â­ |
| **D. ä½¿ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾å› å­** | â­â­ | âœ… | é«˜ | â­â­â­â­ |

---

## æ–¹æ¡ˆ A: ç¼©æ”¾æ–‡æœ¬ç‰¹å¾ï¼ˆæ— éœ€é‡è®­ç»ƒï¼‰â­â­â­

**åŸç†**: æ‰‹åŠ¨æ”¾å¤§ text_transformedï¼Œä½¿å…¶ä¸èŠ‚ç‚¹ç‰¹å¾åŒé‡çº§ã€‚

### å®ç°æ–¹å¼ 1: ä¿®æ”¹ alignn.pyï¼ˆä¸´æ—¶æµ‹è¯•ï¼‰

åœ¨ `models/alignn.py:187` åæ·»åŠ ï¼š

```python
# Transform text features
text_transformed = self.text_transform(text_feat)  # [batch_size, node_dim]

# === ä¸´æ—¶ä¿®å¤ï¼šæ‰‹åŠ¨ç¼©æ”¾ ===
# ç›®æ ‡ï¼šè®© text_transformed çš„ L2 èŒƒæ•°æ¥è¿‘ node_feat
scale_factor = 12.0  # åŸºäºè¯Šæ–­ç»“æœçš„ 12.63:1 æ¯”ä¾‹
text_transformed = text_transformed * scale_factor
```

**ä¼˜ç‚¹**:
- âœ… æ— éœ€é‡æ–°è®­ç»ƒ
- âœ… ç«‹å³ç”Ÿæ•ˆ
- âœ… æ˜“äºæµ‹è¯•ä¸åŒçš„ç¼©æ”¾å› å­

**ç¼ºç‚¹**:
- âŒ ä¸å¤Ÿä¼˜é›…
- âŒ ç¼©æ”¾å› å­æ˜¯ç¡¬ç¼–ç çš„
- âŒ ä¸åŒæ•°æ®é›†å¯èƒ½éœ€è¦ä¸åŒå› å­

### å®ç°æ–¹å¼ 2: åˆ›å»º wrapper è„šæœ¬ï¼ˆæ¨èç”¨äºåˆ†æï¼‰

```python
# create_scaled_model.py
import torch
from models.alignn import ALIGNN, MiddleFusionModule

class ScaledMiddleFusion(MiddleFusionModule):
    """ä¸´æ—¶ä¿®å¤ï¼šç¼©æ”¾æ–‡æœ¬ç‰¹å¾"""

    def __init__(self, *args, scale_factor=12.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.scale_factor = scale_factor

    def forward(self, node_feat, text_feat, batch_num_nodes=None):
        batch_size = text_feat.size(0)
        num_nodes = node_feat.size(0)

        # Transform and scale
        text_transformed = self.text_transform(text_feat) * self.scale_factor

        # ... å…¶ä½™ä»£ç ä¸åŸç‰ˆç›¸åŒ ...
        # (å¤åˆ¶ alignn.py ä¸­çš„ broadcast å’Œ gate é€»è¾‘)

        if num_nodes != batch_size:
            if batch_num_nodes is not None:
                text_expanded = []
                for i, num in enumerate(batch_num_nodes):
                    text_expanded.append(text_transformed[i].unsqueeze(0).repeat(num, 1))
                text_broadcasted = torch.cat(text_expanded, dim=0)
            else:
                text_pooled = text_transformed.mean(dim=0, keepdim=True)
                text_broadcasted = text_pooled.repeat(num_nodes, 1)
        else:
            text_broadcasted = text_transformed

        gate_input = torch.cat([node_feat, text_broadcasted], dim=-1)
        gate_values = self.gate(gate_input)
        self.stored_alphas = gate_values.mean(dim=1).detach().cpu()

        enhanced = node_feat + gate_values * text_broadcasted
        enhanced = self.layer_norm(enhanced)
        enhanced = self.dropout(enhanced)

        return enhanced

# ä½¿ç”¨æ–¹æ³•
def apply_scaling_fix(model, scale_factor=12.0):
    """å°†æ¨¡å‹ä¸­çš„ MiddleFusionModule æ›¿æ¢ä¸ºç¼©æ”¾ç‰ˆæœ¬"""
    for name, module in model.named_children():
        if isinstance(module, torch.nn.ModuleDict):
            for sub_name, sub_module in module.items():
                if isinstance(sub_module, MiddleFusionModule):
                    new_module = ScaledMiddleFusion(
                        node_dim=sub_module.node_dim,
                        text_dim=sub_module.text_dim,
                        hidden_dim=sub_module.hidden_dim,
                        scale_factor=scale_factor
                    )
                    # å¤åˆ¶æƒé‡
                    new_module.load_state_dict(sub_module.state_dict(), strict=False)
                    module[sub_name] = new_module
    return model
```

**ä½¿ç”¨**:
```bash
# åœ¨æå–æˆ–åˆ†æè„šæœ¬ä¸­åŠ è½½æ¨¡å‹åè°ƒç”¨
model = apply_scaling_fix(model, scale_factor=12.0)
```

---

## æ–¹æ¡ˆ B: æ·»åŠ  LayerNormï¼ˆéœ€è¦é‡è®­ç»ƒï¼‰â­â­â­â­â­

**åŸç†**: åœ¨ gate è¾“å…¥å‰å½’ä¸€åŒ–ï¼Œä½¿èŠ‚ç‚¹å’Œæ–‡æœ¬ç‰¹å¾åœ¨åŒä¸€å°ºåº¦ã€‚

### ä¿®æ”¹ MiddleFusionModule

åœ¨ `models/alignn.py` çš„ `__init__` ä¸­æ·»åŠ ï¼š

```python
def __init__(self, node_dim=64, text_dim=64, hidden_dim=128, num_heads=2, dropout=0.1):
    super().__init__()
    self.node_dim = node_dim
    self.text_dim = text_dim
    self.hidden_dim = hidden_dim

    # Text transformation
    self.text_transform = nn.Sequential(
        nn.Linear(text_dim, hidden_dim),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(hidden_dim, node_dim)
    )

    # === æ–°å¢ï¼šGate è¾“å…¥å½’ä¸€åŒ– ===
    self.gate_norm = nn.LayerNorm(node_dim * 2)

    # Gate mechanism
    self.gate = nn.Sequential(
        nn.Linear(node_dim + node_dim, node_dim),
        nn.Sigmoid()
    )

    self.layer_norm = nn.LayerNorm(node_dim)
    self.dropout = nn.Dropout(dropout)
    self.stored_alphas = None
```

åœ¨ `forward` ä¸­ä½¿ç”¨ï¼š

```python
def forward(self, node_feat, text_feat, batch_num_nodes=None):
    # ... (å‰é¢ä¸å˜)

    # Gated fusion
    gate_input = torch.cat([node_feat, text_broadcasted], dim=-1)

    # === æ–°å¢ï¼šå½’ä¸€åŒ– ===
    gate_input = self.gate_norm(gate_input)

    gate_values = self.gate(gate_input)

    # ... (åé¢ä¸å˜)
```

**ä¼˜ç‚¹**:
- âœ… ä»æ ¹æœ¬ä¸Šè§£å†³å°ºåº¦ä¸åŒ¹é…
- âœ… å¯¹æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰æ•ˆ
- âœ… ç†è®ºä¸Šæ­£ç¡®

**ç¼ºç‚¹**:
- âŒ éœ€è¦é‡æ–°è®­ç»ƒ
- âŒ è®­ç»ƒæ—¶é—´é•¿

---

## æ–¹æ¡ˆ C: ä¿®æ”¹ text_transform åˆå§‹åŒ–ï¼ˆéœ€è¦é‡è®­ç»ƒï¼‰â­â­â­â­

**åŸç†**: è®© text_transform è¾“å‡ºæ›´å¤§çš„å€¼ã€‚

### ä¿®æ”¹åˆå§‹åŒ–

åœ¨ `models/alignn.py` çš„ `__init__` åæ·»åŠ ï¼š

```python
def __init__(self, node_dim=64, text_dim=64, hidden_dim=128, num_heads=2, dropout=0.1):
    super().__init__()
    # ... (åŸæœ‰ä»£ç )

    self.text_transform = nn.Sequential(
        nn.Linear(text_dim, hidden_dim),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(hidden_dim, node_dim)
    )

    # === æ–°å¢ï¼šæ”¾å¤§ text_transform çš„æƒé‡ ===
    with torch.no_grad():
        for layer in self.text_transform:
            if isinstance(layer, nn.Linear):
                # æƒé‡æ”¾å¤§ 3 å€ï¼ˆæ ¹æ® sqrt(12.63) â‰ˆ 3.55ï¼‰
                layer.weight.data *= 3.5
                if layer.bias is not None:
                    layer.bias.data *= 3.5
```

**ä¼˜ç‚¹**:
- âœ… ç®€å•ç›´æ¥
- âœ… åœ¨è®­ç»ƒå¼€å§‹æ—¶å°±èµ·ä½œç”¨

**ç¼ºç‚¹**:
- âŒ éœ€è¦é‡æ–°è®­ç»ƒ
- âŒ å¯èƒ½å½±å“è®­ç»ƒç¨³å®šæ€§

---

## æ–¹æ¡ˆ D: å¯å­¦ä¹ çš„ç¼©æ”¾å› å­ï¼ˆéœ€è¦é‡è®­ç»ƒï¼‰â­â­â­â­

**åŸç†**: æ·»åŠ ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°æ¥åŠ¨æ€è°ƒæ•´æ–‡æœ¬ç‰¹å¾çš„æƒé‡ã€‚

### å®ç°

```python
def __init__(self, node_dim=64, text_dim=64, hidden_dim=128, num_heads=2, dropout=0.1):
    super().__init__()
    # ... (åŸæœ‰ä»£ç )

    # === æ–°å¢ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å› å­ ===
    # åˆå§‹åŒ–ä¸º 12.0ï¼ˆåŸºäºè¯Šæ–­ç»“æœï¼‰
    self.text_scale = nn.Parameter(torch.tensor(12.0))

    # ... (å…¶ä½™ä¸å˜)

def forward(self, node_feat, text_feat, batch_num_nodes=None):
    batch_size = text_feat.size(0)
    num_nodes = node_feat.size(0)

    # Transform text features
    text_transformed = self.text_transform(text_feat)

    # === æ–°å¢ï¼šåº”ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾ ===
    text_transformed = text_transformed * self.text_scale

    # ... (å…¶ä½™ä¸å˜)
```

**ä¼˜ç‚¹**:
- âœ… è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜ç¼©æ”¾
- âœ… çµæ´»ä¸”ä¼˜é›…
- âœ… å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ text_scale çš„å€¼

**ç¼ºç‚¹**:
- âŒ éœ€è¦é‡æ–°è®­ç»ƒ

---

## æ¨èçš„ä¿®å¤æµç¨‹

### çŸ­æœŸï¼ˆç«‹å³å¯ç”¨ï¼‰ï¼š

1. **æµ‹è¯•ç¼©æ”¾æ•ˆæœ**ï¼ˆ5åˆ†é’Ÿï¼‰

   ä¿®æ”¹ `models/alignn.py:187` æ·»åŠ ï¼š
   ```python
   text_transformed = self.text_transform(text_feat) * 12.0
   ```

2. **é‡æ–°è¿è¡Œåˆ†æ**
   ```bash
   python diagnose_fusion_effectiveness.py ...
   python 3.analyze_text_flow_v2.py ...
   ```

3. **æ£€æŸ¥æ”¹è¿›**
   - æ¯”ä¾‹åº”è¯¥ä» 12.63:1 â†’ æ¥è¿‘ 1:1
   - ä½™å¼¦ç›¸ä¼¼åº¦åº”è¯¥ä» 0.07 â†’ 0.2-0.4
   - Alpha æ ‡å‡†å·®å¯èƒ½ä» 0.029 â†’ 0.05-0.08

### é•¿æœŸï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ï¼š

1. **å®ç°æ–¹æ¡ˆ B + D**ï¼ˆæœ€ä½³ç»„åˆï¼‰
   - LayerNorm ä¿è¯å°ºåº¦å½’ä¸€åŒ–
   - å¯å­¦ä¹ ç¼©æ”¾å› å­æä¾›é¢å¤–çµæ´»æ€§

2. **é‡æ–°è®­ç»ƒæ¨¡å‹**

3. **éªŒè¯æ”¹è¿›**

---

## é¢„æœŸæ•ˆæœ

### ä¿®å¤å‰ï¼ˆå½“å‰ï¼‰:
```
text_transform è¾“å‡º L2: 2.17
èŠ‚ç‚¹/æ–‡æœ¬æ¯”ä¾‹: 12.63:1
ä½™å¼¦ç›¸ä¼¼åº¦: 0.068
Alpha æ ‡å‡†å·®: 0.029
Gate ç›¸å…³æ€§: -0.70 (å¼‚å¸¸)
```

### ä¿®å¤åï¼ˆé¢„æœŸï¼‰:
```
text_transform è¾“å‡º L2: ~25 (ç¼©æ”¾å)
èŠ‚ç‚¹/æ–‡æœ¬æ¯”ä¾‹: ~1:1
ä½™å¼¦ç›¸ä¼¼åº¦: 0.25-0.45
Alpha æ ‡å‡†å·®: 0.06-0.10
Gate ç›¸å…³æ€§: 0.3-0.6 (æ­£å¸¸)
```

---

## å¿«é€Ÿæµ‹è¯•å‘½ä»¤

```bash
# 1. å¤‡ä»½åŸå§‹ alignn.py
cp models/alignn.py models/alignn.py.bak

# 2. ç¼–è¾‘ alignn.pyï¼Œåœ¨ç¬¬ 187 è¡Œåæ·»åŠ ï¼š
#    text_transformed = text_transformed * 12.0

# 3. é‡æ–°è¿è¡Œè¯Šæ–­
python diagnose_fusion_effectiveness.py \
    --checkpoint <your-checkpoint> \
    --root_dir <your-root-dir>

# 4. æŸ¥çœ‹æ”¹è¿›ï¼ˆåº”è¯¥çœ‹åˆ°æ¯”ä¾‹æ¥è¿‘ 1:1ï¼‰

# 5. é‡æ–°è¿è¡Œæ–‡æœ¬æµåˆ†æ
python 3.analyze_text_flow_v2.py \
    --checkpoint <your-checkpoint> \
    --root_dir <your-root-dir>

# 6. å¦‚æœæ•ˆæœå¥½ï¼Œå¯ä»¥ç»§ç»­ç”¨äºæå–å’Œå¯è§†åŒ–
python 1.extract_alpha_final.py ...
python 2.create_paper_alpha_figures.py

# 7. å®Œæˆåæ¢å¤å¤‡ä»½
mv models/alignn.py.bak models/alignn.py
```

---

## é‡è¦æé†’

âš ï¸ **ç¼©æ”¾æ˜¯ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**ï¼Œç”¨äºï¼š
- åˆ†æå½“å‰æ¨¡å‹çš„ alpha å€¼
- ç”Ÿæˆè®ºæ–‡å›¾è¡¨
- éªŒè¯ä¿®å¤æ–¹å‘

âœ… **é•¿æœŸè§£å†³æ–¹æ¡ˆéœ€è¦é‡æ–°è®­ç»ƒ**ï¼Œä½¿ç”¨ï¼š
- æ–¹æ¡ˆ B (LayerNorm)
- æ–¹æ¡ˆ D (å¯å­¦ä¹ ç¼©æ”¾)
- æˆ–ä¸¤è€…ç»“åˆ

---

## ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ï¼Ÿ

1. **text_transform åˆå§‹åŒ–**
   - PyTorch é»˜è®¤ä½¿ç”¨ Kaiming åˆå§‹åŒ–
   - å¯¹äº 64â†’128â†’256 çš„æ‰©å±•ï¼Œå¯èƒ½åå°

2. **ReLU æ¿€æ´»**
   - ä¼šå°†è´Ÿå€¼å½’é›¶
   - é™ä½è¾“å‡ºçš„æ–¹å·®å’ŒèŒƒæ•°

3. **Dropout**
   - éšæœºä¸¢å¼ƒ 10% çš„ç¥ç»å…ƒ
   - è¿›ä¸€æ­¥é™ä½è¾“å‡º

4. **è®­ç»ƒä¸å……åˆ†**
   - å¦‚æœä¸­æœŸèåˆçš„æŸå¤±æƒé‡è¾ƒå°
   - text_transform å¯èƒ½æœªå……åˆ†ä¼˜åŒ–

ç»¼åˆè¿™äº›å› ç´ ï¼Œå¯¼è‡´ text_transform è¾“å‡ºè¿‡å¼±ã€‚
